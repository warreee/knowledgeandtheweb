\documentclass[12pt,a4paper]{article}
% \usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{svg}
\usepackage{float}
\usepackage{url}
\usepackage{hyperref}

\author{
  Katrien Laenen\\
  \and
  Gust Verbruggen\\
  \and
  Ward Schodts
}
\title{Knowledge \& the web: exercise session 6}
\begin{document}
\maketitle
In this report we discuss which algorithms we tried to find a good classification model that predicts if a passenger of the Titanic will survive or not.

We use the data from \texttt{titanic\_train.csv} to train our model and perform cross-validation for performance assessment. The models used are a \textbf{rule set} and a \textbf{decision tree}.  The rest of this document is structured as follows. First we describe the preprocessing steps used in order to prepare the data for better classification. Next, we describe the learning process for both models. Finally, we compare results for both learning techniques, based on the cross-validation results. 

\section{Preprocessing}
In order to achieve better classification, we preprocess the data. This is done in two phases: feature selection to only preserve relevant features and feature engineering in the form of a discretisation step.

\paragraph{Feature selection} Features that we do not think are relevant are \texttt{Name}, \texttt{Ticket}, \texttt{PassengerId}, \texttt{Embarked} and \texttt{Cabin}. They are removed.

\paragraph{Discretisation} The two numerical values are discretised in accordance to some criterium. The \texttt{Age} feature into five age classes: child ($<$ 9 years), teen (10 to 15 years), young (15 to 24 years), adult (25 to 59 years) and elderly ($>$ 59 years). To discretise \texttt{Fare}, we take a look at a histogram plot, as seen in Figure~\ref{fig:histfare}. There are a lot more low fares. The three leftmost bars get their own bin. The chosen discretisation is: 0-10, 10-20, 20-30, 30-100 and 100+.

\begin{figure}[htbp]
	\centering
	\includegraphics[width = .7\textwidth]{fare_histogram.pdf}
	\caption{Histogram for the \texttt{Fare} feature}
	\label{fig:histfare}
\end{figure}


\section{Rule learning}
One thing we tried was to learn classification rules from the training data. Rule learning algorithms come with different options for several aspects of the algorithms. In the exercise session we experimented with two of these.
\par First, for choosing the conditions of a classification rule, there are two options: one is to add to the rule the condition which increases the accuracy the most, another is to add to the rule the condition which increases the information gain the most. The accuracy criterion option resulted in the following rule set:
\begin{itemize}

\item if Parch $>$ 5.500 then 0  (1 / 0)
\item if Fare $\geq$ 10 and Sex = male then 0  (170 / 25)
\item if Sex = female then 1  (63 / 197)
\item else 0  (171 / 61)

\end{itemize}
%which is correct for 539 out of 688 training examples.
The information gain criterion option resulted in this rule set:
\begin{itemize}

\item if Sex = male and Fare $\geq$ 10 then 0  (170 / 25)
\item if Sex = female then 1  (64 / 197)
\item if Fare = 10-20 and Parch $\leq$ 0.500 then 0  (66 / 7)
\item else 0  (111 / 54)

\end{itemize}
%which is correct for 544 out of 694 training examples.
\par A second aspect of rule learning is the desired pureness, which is the minimum ratio of the major class in a covered subset in order to consider the subset pure. The standard value for this is 0.9. We experimented with this value set to 0.8 and 1. For the desired pureness set to 0.8 and the accuracy criterion we got the following rule set:
\begin{itemize}
\item if Parch $>$ 5.500 then 0  (1 / 0)
\item if Fare $\geq$ 10 then 0  (189 / 47)
\item if Sex = female then 1  (44 / 175)
\item else 0 (171 / 61)
\end{itemize}
%which is correct for 536 out of 688 training examples.
\par This rule set is very similar to the rule set obtained with the accuracy criterion option and desired pureness of 0.9. The only difference is that in the second rule the \textit{Sex = male} condition now disappeared from the condition part. 
For the desired pureness set to 0.8 and the information gain criterion we got the following rule set:
\begin{itemize}

\item if Sex = male then 0  (360 / 93)
\item else 1  (57 / 177)

\end{itemize}
%which is correct for 537 out of 687 training examples.
\par When comparing this rule set with the rule set obtained for the information gain criterion with the desired pureness set to 0.9, we see that this rule set is a great simplification of the other. For the desired pureness set to 1 and the accuracy criterion we got exactly the same rule set as when setting the desired pureness to 0.9 for that criterion. The same happened for the desired pureness set to 1 and the information gain criterion, which also resulted in the same rule set as when setting the pureness to 0.9 and keeping the information criterion.
\par Next to the criterion and desired pureness, there are three other parameters for rule learning: sample ratio (which specifies the sample ratio of training data used for growing and pruning), minimal prune benefit (which specifies the minimum amount of benefit which must be exceeded over unpruned benefit in order to be pruned) and use local random seed (which indicates if a local random seed should be used for randomization). We did not experiment with these parameters, but kept the standard values for these, which were 0.9, 0.25 and no respectively.
\par From these results we can clearly see that Sex, Fare and Parch are features which are important for this task. That Sex and Fare were important features was to be expected. When the Titanic collided with an iceberg, women and children and people of the higher ranks were evacuated first. This is reflected in the data as we see that women and people which payed a higher fare are more likely to have survived. That the number of parents and children is also an important feature can be explained as follows. People in need want to stay close to their loved ones at all costs. But how larger a group, how more difficult it was to get on a lifeboat together. Also, little children and old people need much more assistence to even reach a lifeboat. As such, how more children and parents someone had aboard, how less likely that person was to survive.

\section{Decision trees}

Apart from rule set learning, we also experimented with decision trees. Since we are working with a mix of nominal and numeric data, this is a suitable model, because each node can use its own \emph{rule} for splitting on an attribute. We will describe how we tweaked the learning parameters in order to achieve better results. The initial tree is shown in Figure~\ref{fig:tree_initial}.


\begin{figure}[htbp]
  \centering
  \includegraphics[width = .6\textwidth]{tree_initial}
  \captionsetup{width=.8\textwidth}
  \caption{Tree learned with default parameters}
  \label{fig:tree_initial}
\end{figure}

\par The most obvious observation about this initial model is that female passengers with third class tickets are not yet classified very accurately, yet the tree did not split on it. In order to improve on this, we can adjust the splitting criterion to \emph{accuracy}. It will split nodes such that the accuracy of the tree increases. This results in a very sparse and large tree, which is not shown for exactly that reason. By changing the maximal depth of the tree, we can prevent it from getting too large. A maximal depth of 4 was found to provide good results. The result is shown in Figure~\ref{fig:tree_accuracy}.
\begin{figure}[htbp]
  \centering
  \includegraphics[width = .6\textwidth]{tree_accuracy}
  \captionsetup{width=.8\textwidth}
  \caption{Tree learned with \emph{accuracy} splitting criterion and maximum depth of 4.}
  \label{fig:tree_accuracy}
\end{figure}
As we can see, this achieved the desired result of splitting the undecided node in four new nodes, three of which perform clearly better. The final step is improving prediction for males. While its accuracy is quite high, the recall for surviving males is zero. This probably happens because the gain in criterion is not sufficiently high for the pre-pruning step to split the node. By lowering the minimum gain during pre-pruning, we enable the algorithm to split easier on nodes. Because the \texttt{Fare} node is already split until max depth, we expect that only the rightmost node (males) will be further split by this setting. Figure~\ref{fig:tree_final} confirms this assumption and shows the final model. 
\begin{figure}[htbp]
  \centering
  \includegraphics[width = .8\textwidth]{tree_final}
  \captionsetup{width=.8\textwidth}
  \caption{Tree learned with \emph{accuracy} splitting criterion, maximum depth of 4 and minimum information gain of 0.01.}
  \label{fig:tree_final}
\end{figure}
\par This model is pretty intuitive. For females, Class is the most important deciding factor. Amongst the lowest class women, the ones that payed most for their ticket surprisingly have the lowest survival rate. For males, Age is the most important deciding factor. Children have a far higher survival rate than men.

\section{Rule learning vs. Decision trees}

Settings for Rule learning 1:
\begin{itemize}
\item ...
\end{itemize}

Settings for Rule learning 2:
\begin{itemize}
\item ...
\end{itemize}

Settings for Decision tree 1:
\begin{itemize}
\item ...
\end{itemize}

Settings for Decision tree 2:
\begin{itemize}
\item ...
\end{itemize}

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline 
• & Precision & Accuracy & Recall\\ 
\hline 
Rule learning 1 & • & • & • \\ 
\hline 
Rule learning 2 & • & • & • \\ 
\hline 
Decision tree 1 & • & • & • \\ 
\hline 
Decision tree 2 & • & • & • \\ 
\hline 
\end{tabular} 
\caption{Overview}
\end{figure}

\end{document}
