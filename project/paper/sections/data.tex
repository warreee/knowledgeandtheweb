\section{Data}
\label{sec:data}

First, we determine features of people that might influence their probability of winning a Nobel Prize. They are elaborated in Section~\ref{ssec:features}. Next, we research whether and where this information can be found. The main data source are knowledge bases, also known as Linked Open Data in the Semantic Web. We can use SPARQL queries on an endpoint to easily retrieve structured data \cite{wc3SPARQL}. The used knowledge bases are discussed in Section~\ref{ssec:knowledgebases}. Needed information that is not (yet) linked has to be scraped from websites. These are discussed in Section~\ref{ssec:additional}. We use Perl scripts and regular expressions to extract the data from plain HTML text. All the retrieved data and all the used scripts can be found at the webpage for this paper: \url{kaw.wardschodts.ws}.

\subsection{Feature selection}
\label{ssec:features}
In order to determine whether someone will win a Nobel Prize, a set of reasonable features that the model can base its decisions on is needed. A summary of the chosen features is given here. 
\begin{itemize}
\item \textbf{Year of birth.}
Currently the average age of a laureate is 59 \cite{age}. Evidently, this plays a role in the decision if somebody makes a chance for a Nobel Prize. For example, young scientists are far less likely to win an award because they have yet to prove themselves.
\item \textbf{Alma mater.} We suppose that there's a link between the university where one graduated and his chances of winning a Nobel Prize. For example, admission to prestige universities is only granted to the best candidates. In order to extract a usable feature from an alma mater, we use its \emph{award score}, which is based on how many Nobel Prize and Field medal winners it produced. In case someone has studied at multiple universities that have a score higher than 0, the average is used. Unranked universities automatically have a score of 0 (which is an inherent property of how it is calculated), if this is the case for someone with multiple universities, then these universities are omitted from the average.
\item \textbf{Work productivity.} A measure of work productivity can also be useful to include. This is however not trivial to determine, as not many properties are available for the majority of scientists and politicians. In order to solve this, we attempted defining a \textbf{proxy} measure for work productivity for the training data and testing individuals. For the scientists and economists from the training dataset, their number of publications might be an appropriate measure for productivity. For politicians, the number of speeches they make are perhaps a good indication of how productive they are. \emph{Because this is a sensitive subject, Appendix \ref{sec:productivity} provides a thorough discussion of why we chose these features and how the research question is slightly weakened by our assumption. Furthermore, in Section~\ref{ssec:quality} on quality assessment, we show why we deem both to be a reasonable proxy by comparing their distributions.}
\item \textbf{Popularity.} A popularity measure is easier to define. Facebook automatically generates pages for individuals that exist on Wikipedia. Therefore all people that we consider have at least such automatically generated page. The number of likes these pages have can then serve as a measure for popularity. 
\end{itemize}

\subsection{Knowledge bases}
\label{ssec:knowledgebases}

Using SPARQL queries, knowledge bases are very easy to retrieve structured information from. Three knowledge bases that we used are listed and discussed below.

\begin{itemize}
\item{\textbf{DBpedia}} is a Linked Open Data source which contains information for just about everything. Coming straight from Wikipedia, it is updated daily and should thus be up-to-date all the time. Although it can be edited by anyone, data correctness is verified thoroughly by a team of content moderators. For this reason we get a large part of our training dataset from DBpedia. More specifically, we retrieve the year of birth, as well as alma mater and of course their full name. The name mostly serves as an identifier for linking data from multiple sources. Persons for which not all data is found are immediately omitted.

\item{\textbf{Nobelprize.org}} is the most complete Linked Open Data source concerning Nobel Prize winners. From 1901 on, it contains all Nobel Prizes that have been won. Moreover, it is maintained by the official organisation responsible for the Nobel Prize and should thus be complete and correct. Since it makes use of the FOAF and DBpedia ontologies, querying it is very easy. The most important feature that we retrieve from this source is of course whether someone has won a Nobel Prize. One thing missing is a \emph{sameAs:} link to DBPedia, which makes our task of linking it slightly harder. Our approach for merging this data with the other features is discussed in subsection \ref{ssec:merging}.

\item{\textbf{Talk of Europe}} is a Linked Open Data source about the European Parliament. The dataset covers all plenary debates held in the European Parliament between July 1999 and January 2014 and biographical information about the members of parliament. The politicians in this data source are the ones for which we would like to answer the research question. Using a \emph{sameAs:} relation to DBPedia, we can easily query for further information without needing exact name matches.
\end{itemize}

\noindent All SPARQL queries used to gather the needed information are also available at the webpage of this paper.
In appendix \ref{sec:lod} we provide the links to the specific endpoints and also some of our experiences of using Linked Open Data.

\subsection{Additional data}
\label{ssec:additional}

For the work productivity, university rankings and popularity features no linked data exists. We therefore scraped all needed information from webpages and linked them ourselves. This was done using Perl scripts and regular expressions, with varying difficulty for different sources.

\begin{itemize}
\item{\textbf{Shanghai Rankings. }} 	
The webpage at \cite{rankings} gives an overview of the Shanghai Rankings for 2015. 
It also includes  information on the criteria used to build this ranking, which includes the awards ranking. Universities that are not listed have an award ranking of 0. Because of the tabular form, retrieving the data was fairly straightforward.

\item{\textbf{Facebook. }} As mentioned, every person on Wikipedia has an automatically generated Facebook page. Querying Facebook can be done using the Graph API with simple HTTP GET requests \cite{graphAPI}. Specialised queries can be constructed using combinations of GET parameters, for which the results are returned in JSON format. Because of this, out of all non-linked sources, information from Facebook was by far the easiest to gather. One difficulty that is worth mentioning, is the limit of consecutive queries that can be send to the Facebook Graph API.
We noticed that after sending more or less 630 queries, the user token that is needed to retrieve the information expires. If you generate a new token you aren't allowed to immediatly start querying again. You have to wait for at least an hour. 
This is in fact an odd number of queries, if you read the documentation of the Graph API, only 200 queries per hour are allowed \cite{graphAPIlimit}.
For people with multiple pages, the number of likes for the most liked page is used. Because popular names might have pages not related to the scientist we are interested in, some results might be noisy. Such outliers have to be pruned afterwards.

\item{\textbf{Google Scholar. }} Google Scholar provides advanced search options, the most relevant for us is searching by author. In contrary to Facebook this not achieved by combining GET parameters, but by manually constructing the advanced search terms. For example, to search for publications by Albert Einstein the query  \emph{author:"Albert Einstein"} can be used. The resulting page shows the number of results, which we can then parse. Based on manual testing, Google's advanced search performs pretty well for finding only results for a given author. However, as before, people with the same name can give noisy results, so some outlier detection has to be performed. A second difficulty arose when Google appeared to block automated requests. Appendix \ref{sec:googlescholar} gives an overview of some methods we used in attempt to moderately successfully circumvent those restrictions.
\end{itemize}

\subsection{Overview}

The following table gives an overview of which feature information was collected from what sources. Linked Open Data sources are shown in \textbf{bold}.

\begin{table}[H]
\centering
\begin{tabular}{l|l}
	\textbf{\textsc{Feature}} & \textbf{\textsc{Source}} \\ \hline
	\rule{0pt}{4mm}Place of birth & \textbf{DBPedia} \\
	Year of birth & \textbf{DBPedia} \\
	University ranking & \textbf{DBPedia} + Shanghai Ranking 2015\\
	Work Productivity & Google Scholar, \textbf{ToE}\\
	Popularity & Facebook\\
	Nobel Prize& \textbf{Nobelprize.org}
\end{tabular}
\caption{Overview of used features and their respective sources.}
\end{table}

\subsection{Processing and merging}
\label{ssec:merging}
After collecting the data from different sources, it has to be combined in some way. For the most part, this was pretty easy by matching names. The hardest part was matching the universities of the people with those in the Shanghai rankings to retrieve the correct ranking score. We discuss how we did this in subsection 2.5.1.
Another difficulty was getting the universities for the Nobel Prize winners. More on this issue can be found in subsection 2.5.2.

\subsubsection{Matching of universities}
Not all the universities of the people (found on DBpedia) were an exact match with some university from the Shanghai rankings.
When taking a closer look at these, we discovered that some of them actually were in the list of Shanghai rankings, but with different spelling.
For this reason, we first applied some transformations on both the universities of the people and those from the Shangai rankings before matching them. These transformations include : setting everything to lowercase, removing everything in parentheses, removing all punctuation, removing the words 'university', 'college', 'state', 'of' and 'the' and removing all white spaces.
Universities which still not can be found in this way are just expected to have a ranking score of 0.

\subsubsection{Getting the universities for the Nobel Prize winners}

Another problem we encountered was while getting the universities for the Nobel Prize winners. Nobelprize.org does not contain the alma mater for the Nobel Prize winners. Instead, they store the unversity or research institution the Nobel Prize winner was affiliated with when winning the prize. For this reason we had to get the alma maters for the Nobel Prize winners from DBpedia. As Nobelprize.org does not have a \textit{sameAs} link with DBpedia, this also has to be done with simple name matching on the first and last name of the Nobel Prize winner.
However, DBpedia does not have an alma mater for 580 out of the total of 866 Nobel Prize winners. Since we already have to deal with a situation in which we have a lot of negative examples and a few positive examples, we want to use as many Nobel Prize winners as possible. For this reason, we settled for the universities stored on Nobelprize.org for all Nobel Prize winners for whom we could not find their alma mater on DBpedia.
However, there are still 191 Nobel Prize winners for whom we could not find a university both on DBpedia and Nobelprize.org and that we omit for this reason.
